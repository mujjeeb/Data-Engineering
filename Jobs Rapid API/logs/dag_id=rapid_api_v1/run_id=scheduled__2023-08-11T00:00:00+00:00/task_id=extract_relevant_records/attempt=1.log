[2023-08-16T09:07:04.401+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: rapid_api_v1.extract_relevant_records scheduled__2023-08-11T00:00:00+00:00 [queued]>
[2023-08-16T09:07:04.410+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: rapid_api_v1.extract_relevant_records scheduled__2023-08-11T00:00:00+00:00 [queued]>
[2023-08-16T09:07:04.411+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-08-16T09:07:04.412+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 6
[2023-08-16T09:07:04.413+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-08-16T09:07:04.423+0000] {taskinstance.py:1304} INFO - Executing <Task(PythonOperator): extract_relevant_records> on 2023-08-11 00:00:00+00:00
[2023-08-16T09:07:04.429+0000] {standard_task_runner.py:55} INFO - Started process 339 to run task
[2023-08-16T09:07:04.434+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'rapid_api_v1', 'extract_relevant_records', 'scheduled__2023-08-11T00:00:00+00:00', '--job-id', '25', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmpbcz9is_j']
[2023-08-16T09:07:04.435+0000] {standard_task_runner.py:83} INFO - Job 25: Subtask extract_relevant_records
[2023-08-16T09:07:04.500+0000] {task_command.py:389} INFO - Running <TaskInstance: rapid_api_v1.extract_relevant_records scheduled__2023-08-11T00:00:00+00:00 [running]> on host 17c4b1dd2309
[2023-08-16T09:07:04.603+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=mujjeeb
AIRFLOW_CTX_DAG_ID=rapid_api_v1
AIRFLOW_CTX_TASK_ID=extract_relevant_records
AIRFLOW_CTX_EXECUTION_DATE=2023-08-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-08-11T00:00:00+00:00
[2023-08-16T09:07:04.606+0000] {python.py:177} INFO - Done. Returned value was: (['http://www.tmf-group.com', 'http://www.centennialcollege.ca', 'http://www.magnestar.space', None, 'http://citylitics.com', 'http://www.sunlife.com', 'http://www.citigroup.com', 'http://www.spotify.com', 'http://www.sobeys.com', None], ['JIyPXXRFFMsAAAAAAAAAAA==', 'ICQs4CU35c0AAAAAAAAAAA==', 'M0Ub2VqVAjEAAAAAAAAAAA==', '-jOoGcptkLcAAAAAAAAAAA==', 'QLwqf83NE4UAAAAAAAAAAA==', 'lT7kHdPQCmUAAAAAAAAAAA==', 'KbjWT2dGADQAAAAAAAAAAA==', 'g66tjcrQ4LkAAAAAAAAAAA==', '27RUK2qmungAAAAAAAAAAA==', 'aSlrzq-At0sAAAAAAAAAAA=='], ['FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'FULLTIME', 'CONTRACTOR'], ['Data Engineer', 'Data Engineer (ETL)', 'Senior Data Engineer', 'Sr. Data Engineer', 'Data Engineer', 'Senior data engineer', 'Lead Data Engineer - Hybrid', 'Senior Data Engineer - Royalties & Reporting', 'Data Engineer', 'Data Engineer'], ['https://ca.linkedin.com/jobs/view/data-engineer-at-tmf-group-3694365174', 'https://ca.linkedin.com/jobs/view/data-engineer-etl-at-centennial-college-3688121866', 'https://ca.linkedin.com/jobs/view/senior-data-engineer-at-magnestar-inc-3664917483', 'https://ca.linkedin.com/jobs/view/sr-data-engineer-at-sagen-3694038029', 'https://apply.workable.com/citylitics/j/86811EF9A6/', 'https://www.jobillico.com/en/job-offer/sun-life-financial-gfaezn/senior-data-engineer/12479122', 'https://jobs.citi.com/job/mississauga/lead-data-engineer-hybrid/287/53030780640', 'https://ca.linkedin.com/jobs/view/senior-data-engineer-royalties-reporting-at-spotify-3690966360', 'https://jobs.sobeyscareers.com/sobeys/job/Toronto-Data-Engineer-ON-M5V-1X6/1037110700/', 'https://ca.bebee.com/job/20230815-98feb0415e1dde113d8ec119ba952eda'], ["We never ask for payment as part of our selection process, and we always contact candidates via our corporate accounts and platforms. If you are approached for payment, this is likely to be fraudulent. Please check to see whether the role you are interested in is posted here, on our website.\n\nAbout TMF Group\n\nTMF Group is a leading provider of critical administrative services, helping clients invest and operate safely around the world. We provide legal, financial and employee administration through TMF Group’s teams in 120 offices\n\nKEY RESPONSABILITIES:\n\nWe are looking for a Data Engineer to join our talented data intelligence practice. This position will allow a talented and enthusiastic candidate the opportunity to have an immediate impact within the organization. As a Data Engineer, you will work closely with many teams across our company on complex, data warehousing, advanced analytical projects to perform data sourcing, data profiling, and other data manipulation functions. You will assist the data intelligence team in the development of enterprise-class data solutions that serve various purposes. An individual in this role will be called upon to support various operational tasks and support internal or external projects within the company.\n\nWe are looking for self-starters with the skills necessary where you will be directly responsible for addressing business needs through requirements gathering and collaborating on solution reviews to develop end-to-end data solutions including but not limited to data modeling, data delivery pipelines, data visualization, and working through complex problems and solutioning using agile development methodology. We are looking for a data engineer who is an ideal team player.\n\nHere’s what a typical day for you might look like:\n• Working closely with the solution leads on solution design, architecture, and implementation.\n• Perform extraction, transformation, and loading (ETL) OR extraction, loading, and transformation (ELT) of data from a wide variety of data sources using various data engineering tools and methods into a data lake storage or data warehouse.\n• Query and process large data sets and perform data profiling and data quality assessments.\n• Design and implement data solutions for integration across systems that are both financial and operational.\n• Assist in creating database models and architecture design and documentation.\n• Conduct research and development as well as contribute to the long-term positioning of and emerging technologies related to data sourcing, cleansing, and integration.\n• Document and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code.\n• Improve operations by conducting systems analysis, recommending changes in policies and procedures.\n• Investigate and resolve data defects, and performance issues and build data quality rules.\n• Participate in client-facing project activities such as requirements gathering, solution reviews, and explaining technical complexities and business benefits in layperson terms.\n\nEXPERIENCE/QUALIFICATIONS\n\nTMF Group is a fast-paced environment with high demand for a Data Intelligence team. A successful candidate sets a high standard for their delivery of quality solutions. Listed below are the desired experience and skills demonstrated by a successful candidate.\n• Bachelor’s degree in computer science or data analytics or software engineering or mathematics (master’s degree is a plus).\n• 5+ years of experience in data engineering, data ingestion, data warehouse, data modeling, data migration/ingestion assessments.\n• 5+ years of advanced SQL development and database experience (example: Snowflake, Azure SQL Database, Azure Synapse, Redshift, Oracle, SQL Server, MySQL, NoSQL, etc.).\n• 5+ years in a data engineer role or similar delivering enterprise data lakes/lakehouses/warehouses.\n• 5+ years of experience creating, managing, and troubleshooting complex data ingestion pipeline, SQL queries, views, procedures, and functions.\n• 5+ years of experience in advanced SQL development: management of high volume of data, building aggregates, and cubes, tuning complex queries for performance, improving overall data retrieval process, indexing, and query plan analysis, and refactoring.\n• 5+ years of experience in managing and maintaining data integration between different applications and databases using APIs or REST APIs.\n• 5+ years of experience working with geospatial and geography data.\n• 5+ years of experience with data extraction with a wide variety of ETL/ELT tools and processes.\n• 5+ years of experience building data warehouses, data mart, and symantec data layers.\n• 5+ years of experience with scripting in python, power shell, etc.\n• 5+ years of experience with Azure Data Factory, Azure SQL, Azure Synapse, Azure Databricks, ML/AI, Databricks, and ML Ops.\n• Experienced working in the cloud and serverless database solutions (example: Azure, AWS, Snowflake, BigQuery, BigTable, etc.) in combination with any BI Tools (example: Power BI, Tableau, etc.).\n• Experience working with project management applications (example: JIRA, Kimble, Salesforce, etc.).\n• Experience with version control applications (example: Azure DevOps, GitHub, BitBucket, etc.)\n\nCANDIDATE PROFILE\n• Rigorous attention to detail\n• Ability to communicate with internal and external stakeholders and work through data-related inquiries and issues\n• Effectively manage workload with shifting priorities and varying business requirements\n• Excellent problem-solving skills to identify creative ways to achieve solutions\n• Capable of taking responsibility for a task and managing it through completion\n• Excellent verbal and written communication skills\n• Strong accounting background is a plus.\n• Experience in Accounting, Investment management will be beneficial.\n• Experience with Private Equity applications (Investran, Allvue, eFront) Real Estate (Yardi, MRI), Credit servicing (WSO, Black Mountain, Sentry, Loan IQ) technology stacks will be beneficial.\n• Strong understanding of the Private Funds and Capital Markets (Credit/Debt Servicing) industry\n• Fluent in both written and spoken English.\n\nKEY COMPETENCIES\n\nOperational Excellence\n• Demonstrates the ability to make decisions based on insight and knowledge that impact the immediate team\n• Accountable for ensuring high standards of delivery to clients\n• Actively engages with customer to understand needs and will place a high priority on client service and satisfaction\n• Prioritizes clients’ issues and address them accordingly\n• Will keep up to date with business trends/ changes in law that will impact their role\n\nCommercial awareness\n• Understands TMF Groups' business lines and any relevant local regulatory requirements\n• Has a good understanding of how TMF is structured\n• Be able to demonstrate the impact of own actions on immediate team\n\nLeadership and resource management\n• Plans, coordinates and manages internal and external resources to deliver results in a timely, accurate and professional manner\n• Will share knowledge to ensure team targets are met\n• Informally manages expectations of more senior colleagues regularly\n\nInterpersonal skills\n• Manages internal and external stakeholders' expectations with regards to delivery, escalating concerns as appropriate\n• Builds credibility across all stakeholder groups\n• Able to engage with individuals all areas of an organisation on area of specialism\n\nClient excellence\n• Manages expectations so clients always feel valued\n• Proactive in identifying opportunities and seeking solutions\n• Ensures relevant information is obtained and shared as needed\n• Takes responsibility for delivering superior products and services\n• Implements improvements to client service\n•", "(As a unionized employer, Centennial College gives preference to internal candidates as defined in the collective agreement. External candidates will be considered only if there are no qualified internal candidates)\n\nCentennial College recognizes and affirms Diversity, Equity and Inclusion and Indigenous ways of knowing as central to the vibrancy and uniqueness of its learning and working academic mission. We strongly encourage applications from members of Indigenous communities and all equity-deserving groups including Women, Racialized, Persons with Disabilities, and LGBTQ+ communities.\n\nWe also recognize that Centennial is situated on the Treaty Lands of the Mississaugas of the Credit First Nation and pay tribute to their legacy as well as that of all First Peoples that have been and remain present here in Toronto. We recognize that First Peoples come from sovereign Nations and that part of understanding our responsibilities of residing on this territory are understanding the true history, circumstances and legacy of the Treaties signed here (such as the Toronto Purchase, Robinson-Huron Treaty and Williams Treaties) and including pre-contact Treaties and Agreements between sovereign Nations and that all peoples in this area are therefore Treaty people with obligations and responsibilities to all our relations.\n\nPosition Summary\n\nWe are seeking a skilled Data Engineer to join our team and take responsibility for the design, construction, and maintenance of our data infrastructure. You will collaborate with cross-functional teams to ensure the secure and efficient collection, processing, and storage of data. Your expertise in database design, Extract-Transform-Load (ETL) processes, and big data technologies will be instrumental in enabling data-driven decision-making.\n\nThis role will work with the business to improve user understanding of the potential for taking full advantage of information available to them to meet the College's business plans. This will include expanding the data warehouse and capabilities by extending and/or improving the architecture and other external data sources with the primary focus to design and administer the ETL.\n\nThe incumbent is an expert with all aspects of Business Intelligence (BI) including the collection and transformation of detailed data elements to help users access and use the information in the data warehouse; assist BI Analysts in developing presentation layer(s) and providing specialized reporting views; and support clients by providing features\\data elements for advanced analysis such as forecasting and predictions. As a subject matter expert in data modeling, ETL, SQL Programming, Data Warehousing and Relational Database Management System (RDBMS), you will work with clients to analyze, plan and organize analytic databases and applications.\n\nResponsibilities\n\nProject Leadership and Support\n\n• Leads the development and maintenance of data warehouses, Operational Data Stores (ODS) and Enterprise Data Warehouse (EDW), and creates new or expanded data stores to support new requirements.\n\n• Leads multifunctional teams to ensure data warehouse projects are aligned with other Information Technology (IT) projects and meet user requirements.\n\n• Resolves issues related to service requests or production problems.\n\n• Liaises and communicates with all levels of college management and other project teams to coordinate efforts.\n\n• Collaborates with business users to understand and collect business requirements.\n\n• Supports the coaching and training of staff on processes, procedures, and appropriate work practices, as needed.\n\nPlanning and Analysis\n\n• Plans projects to develop new data sources, new data feeds, new data stores and new tools for end users.\n\n• Analyzes current reporting applications and/or data warehouses through review of documentation and interviews with clients.\n\n• Follows and is compliant with company policies and procedures and the Freedom of Information and Protection of Privacy Act (FIPPA).\n\n• Prepares or assists with feasibility studies, requirements definitions (e.g., Source to Target Mappings), conceptual design documents, work plans, systems design, implementation designs, system and program specifications, and system and client documentation.\n\n• Troubleshoots production systems.\n\n• Develops and manages implementation of data/code fixes for defects utilizing the Application Lifecycle Management (ALM) process.\n\n• Monitors data warehouses load performance and data quality on a regular basis or for month-end and works with IT staff to improve both performance and quality.\n\n• Implements small source system changes within databases that are not part of College Projects such as new products and cover codes.\n\nDesign, Development and Administration\n\n• Leads and is a hands-on developer on warehouse projects, using a deep understanding of the Enterprise Resource Planning (ERP) product; flow of information in context of data warehousing; and Oracle technologies including Oracle ODI.\n\n• Coordinates the design and development of efficient workflow processes to automate ETL events; designs end-to-end processes to extract, transfer, and load data from the Banner ERP solution and other sources into the custom and ODS data marts.\n\n• Builds data pipelines to extract data from various sources, transform it into a usable format, and load it into storage systems.\n\n• Fine-tunes and performs performance enhancement throughout all ETL processes through data cleansing recommendations, coding and redesigning of data models.\n\n• Designs and implements database schemas and data models to ensure data is organized, consistent, and easily accessible.\n\n• Ensures data accuracy and consistency by performing data quality checks and implementing appropriate validation processes.\n\n• Implements and maintains data security measures to protect sensitive information from unauthorized access.\n\n• Performs system upgrades of the data warehouse in coordination with systems and platform upgrades.\n\n• Monitors, identifies, troubleshoots and reports application performance and outages.\n\n• Develops SQL queries, shell scripts, PL/SQL stored procedures, and reports using customer requirements and documents, in accordance with established criteria and standards.\n\n• Designs, integrates, tests, maintains, and supports automated reports delivered through Cognos or other technologies, as required.\n\n• Prepares operational documentation for the system users and support teams according to established departmental and divisional standards.\n\nQualifications/Experience\n\n• 4-year University Degree or a post-Graduate Degree with a focus on Computer Science, or a related field.\n\n• Oracle SQL and PL/SQL, Shell scripting\n\n• Extract-Transform-Load Tools and Techniques (Oracle Database Integrator or similar)\n\n• Fundamentals of Project Management\n\n• Data Warehouse Design (Kimball and/or Inmon methodologies or similar)\n\nMinimum 5 years of experience as:\n\n• Data Warehouse Design and Architecture; designing, implementing and supporting data model and data warehouse projects.\n\n• A Business Analyst and/or Systems Analyst in information technology, together with experience in managing customer relationships and project coordination/management.\n\nExperience with:\n\no Technology process mapping, business process mapping and workflow.\n\no Technical Project Leadership.\n\no Oracle technologies including Oracle PL/SQL.\n\no Extract-Transform-Load Tools and Techniques (Oracle Database Integrator or similar).\n\no A Student information System, and Portal technology (authentication, distribution, group collaboration and channel management).\n\no Data modeling toolset such as ERWIN or ER Studio Data Architect\n\no Internet Information Servers (IIS), WINDOWS, Linux\\UNIX.\n\no Big Data technologies, such as Hadoop, Spark, etc.\n\no Cloud platforms, such as AWS, Azure, or Google Cloud Platform\n\n• Experience executing quality assurance best practices and procedures for information systems and services.\n\n• Strong knowledge and experience in Shell scripting.\n\n• Some of: knowledge of Oracle database architecture; Hyper Text Mark-up Language (HTML), Dynamic Hyper Text Mark-up Language (DHTML); JAVA, Extended Mark-up Language (XML), Groovy, Shell Scripting, Python; APACHE, Simple Mail Transfer; and/or Protocol (SMTP).\n\n• Skills in Microsoft Office, Microsoft Project, and Microsoft Visio, Microsoft SharePoint.\n\n• Experience in designing and implementing data pipelines using ETL tools.\n\n• Strong problem-solving and analytical skills.\n\nApply online: www.centennialcollege.ca/careers\n\nProof of credentials or equivalencies from accredited regional or federal post secondary institutions and/or their foreign equivalents will be required at the time of job offer.\n\nWhen applying, your cover letter and résumé must include examples that reflect all of the requested skills and qualifications and must be submitted online by September 1 at 11:59 PM EST. Please quote Job ID J0723-0503. Misrepresentation of applicant information will be grounds for your exclusion from the competition or for dismissal should you subsequently be hired for the position. We wish to thank all applicants for their interest and advise that only those selected for an interview will be contacted.\n\nWe are committed to providing persons with disabilities equal opportunities regarding all employment activities, including access to jobs and accommodations during employment as required, in accordance with the Ontario Human Rights Code (OHRC) and the Accessibility for Ontarians with Disabilities Act (AODA).", 'Location: Hybrid WFH/Toronto - Remote-only may be considered for the right candidate. Some travel may be required.\n\nHead office: Toronto, ON.\n\nThree weeks paid vacation, health benefits programs, equity.\n\nOur ideal candidate has a wide breadth of technical skills, is hungry to solve exciting, complex data problems, and is willing to practice Development & Operations of technology.\n\nThis is a senior-level position for an experienced data engineer looking to contribute their programming expertise to revolutionize the space industry. We are looking for a technical superstar, aiming to work on a world changing problem.\n\nWhile prior experience in the space sector is not a requirement, we are looking for an individual with a desire to learn and understand the business domain. The person originating this role will be responsible for the design, development, implementation, and maintenance of robust middleware solutions to facilitate seamless integration of the applications developed by Magnestar.\n\nResponsibilities include:\n• Modelling distributed data, creating different data models, understanding how models are produced and the process within a complex database system.\n• Integrate data across systems to become one.\n• Looking at data like a manufacturing process.\n• Optimizing data architecture and performance to achieve scalability, reliability, and high availability\n• Designing connectivity architecture and building into the software ecosystem.\n• Troubleshooting and resolving infrastructure issues, including network and security-related challenges, to ensure optimal system performance and data integrity.\n• Creating comprehensive technical documentation, including design specifications, implementation guidelines, and troubleshooting procedures.\n\nQualifications\n• Technical superstar\n• Tons of technical skills, maturity in data modelling, ability to pull in complex data, fix data, build APIs, etc.\n• 15+ years of experience in programming\n• In-depth knowledge of middleware technologies, protocols, and integration patterns.\n• Proficiency in troubleshooting complex infrastructure issues, including network and security concerns.\n• Strong programming skills with experience or interest in relevant languages particularly Python, C++, or Rust - however we are open to other options depending on expertise.\n• Familiarity with cloud technologies and platforms, such as AWS, GoogleCloud, Azure and Private networks\n• Can-do person, extremely excited by being able to dive into data systems.\n• Proactive, keep moving towards goals.\n• Demonstrates keen interest in working in space industry\n• Ability to pass security clearance requirements if necessary\n\nAbout Magnestar\n\nhttps://www.magnestar.space/\n\n250 University Avenue - Toronto, ON.\n\nMagnestar was created to ensure the resiliency of communications between Earth and space. Magnestar is in the process of building a clearing house for signal interference, which ensures clear pathways of communications for all space operators.\n\nFounded in December 2021, Magnestar has had over 500 people apply to work at our company, completed pre-seed funding, secured contracts, grown the company network by over 10,000 people, and won multiple early-stage space industry awards.\n\nMagnestar is aiming to add 4 new team members by the end of 2023. Magnestar employs a diverse, multi-generational group of individuals with expertise in multiple sectors. The space industry moves extremely fast and our working environment reflects that pace. We love to solve complex problems and are always supportive of one another. We value those who want to learn and we always remember to celebrate our wins as a team.', "Job Summary\n\nIn collaboration with the Manager, Data Operations , the Senior Data Engineer is responsible for the data, architecture, and model of our Enterprise Data & Analytics environment. This role will maintain the current data warehouse solution and be an active participant in decommissioning legacy solutions.\n\nResponsibilities\n• Plan, coordinate, develop and support data model development, including architecting table structure, building ETL process, documentation, and long-term preparedness.\n• Analyze, design, develop, implement, and maintain technical solutions in AWS using data ETL technologies, including Amazon S3, AWS Glue Studio, AWS Glue Data Catalog, Lake Formation, Glue crawlers, AWS Athena, and Snowflake.\n• Maintain the current data warehouse solution and be an active participant in decommissioning legacy solutions.\n• Develop cross-validation rules to ensure data accuracy.\n• Define technical approach documents (define calculated logic, process flow).\n• Meet business requirements of the projects as new data sources are added.\n• Provide production support for existing data pipelines.\n• Document steps needed to migrate database objects between various environments, including Development, UAT, Staging, and Production.\n• Provide regular project updates to shared team resources and management as needed.\n• Communicate issues, risks, and concerns proactively to manage and engage colleagues while working in a hybrid Agile/Scrum process.\n• Document pipelines thoroughly to allow peers to assist with support as needed.\n• Provide technical assistance and task delegation to a flexible team of staff engineers, consultants, and interns in close coordination with the team Manager and Data Architect.\n• Nights, weekends, and non-regular hours may be required to deploy and make ready solutions that need support and collaboration from resources.\n\nSkills And Qualifications\n• Bachelor's degree in computer science or related field and five plus years’ experience in data engineering and data warehouse technologies with a focus on the AWS platform in the past five years to present.\n• A minimum of 5 + years’ experience in progressive data engineering roles.\n• Expert knowledge in SQL, ETL architecture, data architecture and BI architecture.\n• 5+ years’ experience in Python for data and analytics.\n• Experience with Amazon S3, AWS Glue Studio, AWS Glue Data Catalog, Lake Formation, Glue crawlers, AWS Athena, and Snowflake or could data warehouse.\n• Knowledge of best practices in enterprise-wide database implementation, availability, and security configuration.\n\nQualifications/Experience Preferred:\n• Experience in migrating from Informatica to AWS.\n• Informatica, POWER BI, and Alteryx experience.\n• Previous experience with Snowflake implementation.\n• Knowledge using AWS EMR and Sagemaker.\n• RESTful API experience.\n• Data pipeline experience (e.g., Kafka).\n• Statistical software experience (e.g., JMP, Minitab).\n\nS agen is committed to creating a diverse and inclusive work culture that closely matches the diversity of our customers. We encourage applications from all backgrounds and abilities to ensure we get the best, most creative and diverse talent across our business.\n\nSagen will provide accommodations to applicants with disabilities throughout the selection process to meet their individual needs.\n\nAs an Employer Partner of the Canadian Center for Diversity and Inclusion and a member of the Black North Initiative, Sagen encourages you to apply and join a fantastic organization.", "About Citylitics\n\nCitylitics delivers predictive intelligence on local utility & public infrastructure markets\n\nWhat is Infrastructure? It is the roadways you rely on to safely get to Grandma's house, it's the potable water that comes out of your kitchen tap that you wash your family's food with and it's the energy that heats our homes and powers our digital lifestyles.\n\nEvery year, trillions of dollars are spent on all areas of infrastructure to maintain our quality life and move our economy forward. However, our infrastructure is no longer equipped to meet the needs of the future. We hear about infrastructure failures, whether bridge collapses, power blackouts, or water main breaks, every day in the news. Climate change and extreme weather events are disrupting the basic infrastructure we took for granted for years.\n\nCitylitics is solving the hardest data problems in infrastructure while building the market & opportunity intelligence platform that enables a faster, more transparent, and more efficient infrastructure marketplace. We turn millions of unstructured documents into high value intelligence feeds and datasets that are available on an intuitive user experience. Our goal is to enable solution providers to connect with cities with relevant infrastructure needs in a faster and more digital way than historic market channels. As more companies adopt our platform, cities & utilities will be able to access solutions that deliver on the promise of moving towards a more resilient, sustainable, and equitable infrastructure future.\n\nWhat are we looking for?\n\nWe are looking for a Data Engineer with an ownership mindset where they can take a challenging data problem all the way through to an elegant tech solution that is robust and scalable. Someone who prides themselves on not only writing efficient code, but also in creating intuitive dashboards and data flows with clear and easy to read documentation. You will ship major product initiatives that fundamentally change the way that people see infrastructure data. This type of ownership will provide incredible opportunities for personal and professional growth. You will have ample technical and team leadership opportunities, depending on your interests.\n\nWhat will you accomplish?\n• Design and implement scalable data pipelines that power our products and all the market insights that our customers love\n• Spearhead the development of end-to-end data management and customer-facing dashboards. Observe and foster best practices in business process design, implementation, and review.\n• Integrate (ingest, model, load, transform) data from external, heterogeneous sources and combine them with internal data to produce insights\n• Automate internal data processes and tooling\n• Developing Python code to facilitate the integration of additional data sources\n• Collaborating in cross-functional software/architecture design sessions to find the best data sessions for the problems that we are facing\n• Develop and maintain our data warehouse ETLs and automated report scheduling system\n• Create reports and data pipelines for stakeholders across the organization from multiple sources\n• Manage and launch new data pipelines, dashboards and machine learning models to accelerate our data strategy and roadmap\n• Other duties as assigned\n\nTechnologies we use:\n• Backend: Python, Django, Cloud SQL and Airflow/Cloud Composer as the main language, web framework, database and orchestration tool respectively\n• Cloud Infrastructure: Google Cloud Platform and Docker\n• Other Tools: Javascript, React as the main language and framework for our frontend\n• 2+ years experience with Python and Airflow/Cloud Composer\n• 2+ years experience with database design and development\n• 1+ year experience with Google Cloud Platform, Docker\n• Experience with Django is an asset, with a particular focus on Django migration\n• This is a rare opportunity to influence positive change within one of the biggest societal challenges of our generation (infrastructure - from water to transport to energy and more).\n• Be a part of a scale up company with no corporate bureaucracy here. You will accomplish more here in a few months than what you would in a few years at a large, entrenched technology company.\n• Comprehensive health and dental benefits with an emphasis on mental health\n• Annual Learning and Development Budget and Plans\n• Flexible hours and work-from-wherever you are with the option to work in our downtown office if interested!\n• Competitive salary and equity incentives to give you a stake in our future\n• We work hard. We play hard. From virtual trivia to exploring the food scene around our office, we like to get together!", "You are as unique as your background, experience and point of view. Here, you’ll be encouraged, empowered and challenged to be your best self. You'll work with dynamic colleagues - experts in their fields - who are eager to share their knowledge with you. Your leaders will inspire and help you reach your potential and soar to new heights. Every day, you'll have new and exciting opportunities to make life brighter for our Clients - who are at the heart of everything we do. Discover how you can make a difference in the lives of individuals, families and communities around the world.\n\nJob Description:\n\nRole Summary:\n\nCome be part of an exciting and challenging opportunity by helping to accelerate the growth and application of data analytics capabilities at Sun Life Canada. As part of the Canada Finance Reporting Centre of Excellence, you will have the opportunity to be at the forefront of applying data and analytics to transform our financial reporting processes. You will be helping Sun Life use analytics to deliver on our purpose of helping our Clients achieve lifetime financial security and live healthier lives.\n\nAs a Senior Data Engineer on the Canada Finance Reporting COE, your focus will be working with a team of financial professionals to build out a robust data environment to enable a suite of impactful business intelligence products and solutions to support financial operations within Canada. These products and services will allow Sun Life Canada’s Finance leaders to better analyze financial drivers and make data driven decisions.\n\nWhat will you do?\n• Develop deep understanding of enterprise data, to enable end-to-end solutions\n• Design and engineer data models with key financial business metrics (eg. Expenses, FTE, Sales)\n• Maintain high level of data quality and integrity across data sources\n• Convert complex business and technical rules into logic for data flows and data pipelines\n• Collaborate with stakeholders and design data solutions for various use cases\n• Standardize metadata into a common glossary with the necessary documentation\n\nWhat do you need to succeed?\n\nCode and tools\n• 3-5 years hands-on experience working on AWS technologies for data processing and analytics\n• AWS Lambda, Glue, SQS, SNS, Cloudwatch or other Cloud technologies\n• Python, Pyspark and SQL\n• Proven track record in leveraging SQL and SQL-based programming to solve business problems\n\nData Modelling\n• Experience with relational database systems, relational models, dimensional models etc.\n• Sound understanding of data management principles (data warehousing, quality, master data management, etc.)\n• Sound understanding of data modelling and a passion for analytics\n• Experience with engineering data products as input to various analytical models\n• Ensure feedback loops between model deployment and its data - i.e. tune & tweak data products to achieve scale, optimization, etc.\n\nProblem Solving\n• Ability to translate complex business requirements into a set of data ingestion pseudocode\n• Proven ability to leverage knowledge of data engineering to extract, conform and integrate a variety of operational data sources into production-grade data products\n• Demonstrate blend of tenacity, creativity, and discipline required to develop ground breaking self-serve data models for consumption\n\nEducation and Qualifications:\n• Undergraduate degree in Computer Science, Mathematics, Engineering, or equivalent\n• Graduate degree in business or quantitative science strongly preferred\n\nUnique Requirements\n• The candidate selected for this role is required to attain Canadian Reliability Security Clearance (administered by submitting fingerprints to the RCMP, who then conduct min. 5 year history checks)\n• To see if you are eligible for this clearance, please review the section 201 on the Federal Government site (https://www.tpsgc-pwgsc.gc.ca/esc-src/personnel/pdcf-rsrp-eng.html)\n\nWhat's In It For You?\n• Competitive salary and bonus structure influenced by market range data\n• 20 days vacation per year and an innovative sabbatical program\n• Flex hours and Flexible hybrid work model including in-country work-from-home if you prefer. #LI-Hybri\n• A friendly, collaborative and inclusive culture\n• Being part of our Analytics community, where we share best practices and broaden skill-sets\n• Flexible Benefits from the day you join to meet the needs of you and your family\n• Pension, stock and savings programs to help build and enhance your future financial security\n• Fitness and wellness programs that help you balance work and life and enjoy a healthier lifestyle\n\nThe Base Pay range is for the primary location for which the job is posted. It may vary depending on the work location of the successful candidate or other factors. In addition to Base Pay, eligible Sun Life employees participate in various incentive plans, payment under which is discretionary and subject to individual and company performance. Certain sales focused roles have sales incentive plans based on individual or group sales results.\n\nDiversity and inclusion have always been at the core of our values at Sun Life. A diverse workforce with wide perspectives and creative ideas benefits our clients, the communities where we operate and all of us as colleagues. We welcome applications from qualified individuals from all backgrounds.\n\nPersons with disabilities who need accommodation in the application process or those needing job postings in an alternative format may e-mail a request to thebrightside@sunlife.com.\n\nAt Sun Life we strive to create a flexible work environment where our employees are empowered to do their best work. Several flexible work options are available and can be discussed throughout the selection process depending on the role requirements and individual needs.\n\nWe thank all applicants for showing an interest in this position. Only those selected for an interview will be contacted.\n\nSalary Range:\n74,100/74 100 - 120,800/120 800\n\nJob Category:\nAdvanced Analytics\n\nPosting End Date:\n27/08/2023", 'This is an exciting opportunity for a Lead, cloud-native Java/python or full-stack software engineer empathetic with the challenges that Data Engineering teams face in delivering software in large, heterogeneous organisations. If you are passionate about engineering excellence and building the best developer experience into your solutions, come and build a meaningful career at Citi and help thousands of developers.\n\nThe Role:\n\nThe Institutional Clients Group (ICG) Engineering and Architecture Practice (EAP) is responsible for defining and executing core architecture and technology strategy for ICG and works within ICG to ensure complete strategic alignment and efficiency of execution across a diverse portfolio of engineering activities, including external Cloud Capability development, Platform Architectures, and Data Engineering. This role is for a strong Lead Data Engineer in the ICG Data Engineering team who will be responsible for codification of the architecture, designing new system capabilities and building data engineering components on-premise and on cloud. Your solutions will be leveraged by tens of thousands of developers across Citi supporting applications used by hundreds of thousands of internal and client users, and must be simple to use, well documented with an excellent developer experience. Engineering excellence and a hands-on background in modern software engineering are essential. Inclusivity at Citi means your primary responsibility is to the team: supporting a diverse team, collaborating to further develop as engineers, doing satisfying and valuable work, and enjoying doing it. You will be expected to positively influence our culture and collaborate with the team on strategy and technical delivery.\n\nWhat you’ll be doing:\n• This is a hands-on technical role, building solutions that will be leveraged by developers. You will demonstrate sound engineering principles and a good understanding of modern CI/CD toolsets.\n• Advocate and advance modern software development practices, including privacy and security first principles. Incorporate DevSecOps and SRE best practices, helping engineering teams navigate complex security and risk requirements and architect innovative solutions that are secure, resilient and scalable.\n• Ensure that your team delivers great solutions that users love by maintaining a culture of quality and engineering excellence.\n\nWho you are:\n• You are an experiencedsoftware engineer with a passion for building modern, cloud-native, fully observable data engineering solutions. You strive to build solutions that are valued by developers and have an experience-based understanding of what that means.\n• You have solid experience in an agile development environment with modern programming languages and technologies such as Golang, Java/Spring Boot, Node.JS, Kotlin, Scala, Python etc.\n• You have experience with implementing cloud-native applications using open and closed source standards and solutions such as Docker, Kubernetes, RedHat OpenShift, Cloud Foundry.\n• You have experience with public cloud providers such as AWS (preferable), Azure and GCP.\n• You have a good working knowledge of microservices architectures, Rest APIs, streaming and message queueing systems (Kafka etc.) and how to instrument them.\n• You love working together on scalable systems, embracing new technologies, and pushing the boundaries of “the art of the possible”.\n• Expertise in Big Data technologies and cloud migration projects is a plus.\n\nCiti Canada is an equal opportunity employer. Accordingly, we will make accommodations to respond to the needs of people with disabilities (including, without limitation, physical and mental health disabilities) during the recruitment process and otherwise in accordance with law. Individuals who view themselves as Aboriginals, members of visible minority or racialized communities, and people with disabilities are encouraged to apply.\n\n-------------------------------------------------\n\nJob Family Group:\nTechnology\n\n-------------------------------------------------\n\nJob Family:\nSystems & Engineering\n\n------------------------------------------------------\n\nTime Type:\nFull time\n\n------------------------------------------------------\n\nCiti is an equal opportunity and affirmative action employer.\n\nQualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\n\nCitigroup Inc. and its subsidiaries ("Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.\n\nView the "EEO is the Law" poster. View the EEO is the Law Supplement.\n\nView the EEO Policy Statement.\n\nView the Pay Transparency Posting', "We are looking for an engaged and enthusiastic Senior Engineer to join our Royalties & Reporting (RoaR) teams in the United States. We are responsible for designing, building, and maintaining complex systems to process billions of data points from Spotify’s internal datasets to accurately calculate and report publishing royalties. The team is cross-functional, consisting of Data and Backend Engineers, and is well supported by a skilled Product Owner.\n\nOur core challenge is to understand and model a complex business domain. Our systems must be flexible enough to allow Spotify to innovate while at the same time simple enough to enable us to build solutions that follow the best engineering practices and can be easily understood.\n\nWhat You'll Do\n• Design and develop new functionality using the latest technology (Scala, Scio, Java, Flyte, Styx, Docker, etc.) to enable innovative ways of compensating artists.\n• Build and operate infrastructure, toolset, and deployment pipelines\n• Advocate for the best architecture standards, drive sound technical decisions, and promote innovative solutions in an iterative, fail-fast environment.\n• Work in an encouraging team that allows engineers to be creative and seek exciting ideas.\n• Collaborate closely with your teammates and leads. You will contribute to building a solid team with high-quality output and create a culture of innovation and playfulness.\n• Work closely with the product manager, end-users, and business partners to understand, document, solve and analyze requirements for complex data solutions.\n• Financial legislation covers our systems, and you will help us improve our automated change management controls.\n• Hack on what you want during regular team hack days and annual Spotify hack weeks.\n\nWho You Are\n• You have proven experience in building production-quality data solutions with complex business domain logic in high-level programming languages (Scala, Java, Python, etc.)\n• Comfortable working with Google Cloud Platform or other commercial cloud environments.\n• Passionate about Data design, reliability, and quality.\n• You know and care about sound engineering practices like CI/CD, defensive programming, and automated testing.\n• Have experience working with both technical and non-technical customers.\n• A strong analytical problem solver, comfortable taking on loosely defined problems, and not afraid to experiment with new technologies to tackle our customer’s problems.\n• You are a self-motivated individual contributor and phenomenal teammate who can multitask, prioritize and communicate progress in a constantly evolving environment.\n• Interested in building skills to improve your T-shapeness with analytics and backend engineering.\n• Knowledgeable about modern infrastructure and tools.\n• You have experience working in an agile environment.\n• Have a Master's degree in Computer Science or relevant working experience.\n• Collaborate well in a distributed team and make time to have fun with your colleagues.\n\nWhere You'll Be\n• We are a distributed workforce enabling our band members to find a work mode that is best for them!\n• Where in the world? For this role, it can be within the Americas region in which we have a work location.\n• Prefer an office to work from home instead? Not a problem! We have plenty of options for your working preferences. Find more information about our Work From Anywhere options here.\n• Working hours? We operate within the Eastern Standard time zone for collaboration.\n\nThe United States base range for this position is $156,275 - $223,250, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, 23 paid days off, 13 paid flexible holidays, paid sick leave. This range encompasses multiple levels. Leveling is determined during the interview process. Placement in a level depends on relevant work history and interview performance.\n\nSpotify is an equal opportunity employer. You are welcome at Spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. Our platform is for everyone, and so is our workplace. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! So bring us your personal experience, your perspectives, and your background. It’s in our differences that we will find the power to keep revolutionizing the way the world listens.\n\nSpotify transformed music listening forever when we launched in 2008. Our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. Everything we do is driven by our love for music and podcasting. Today, we are the world’s most popular audio streaming subscription service.", "Requisition ID: 173769\n\nCareer Group: Corporate Office Careers\n\nJob Category: Advanced Analytics\n\nTravel Requirements: 0 - 10%\n\nJob Type: Full-Time\n\nCountry: Canada (CA)\n\nProvince: Ontario\n\nCity: Greater Toronto Area\n\nLocation: Sobeys Innovation Hub\n\nPostal Code:\n\nOur family of 134,000 employees and franchise affiliates share a collective passion for delivering exceptional shopping experiences and amazing food to all our customers. Our mission is to nurture the things that make life better – great experiences, families, communities, and our employees. We are a family nurturing families.\n\nA proudly Canadian company, we started in a small town in Nova Scotia but we are now in communities of all sizes across this great country. With over 1500 stores in all 10 provinces, you may know us as Sobeys, Safeway, IGA, Foodland, FreshCo, Thrifty Foods, Lawtons Drug Stores or another of our great banners but we are all one extended family.\n\nAll career opportunities will be open a minimum of 5 business days from the date of posting.\n\nOverview\n\nWe are looking for a highly qualified Data Engineer to join our team and create and maintain optimal data pipeline architecture. In this role, you will be responsible for building infrastructure, identifying internal process improvements, and working closely with analytics and business teams to improve data models.\n\nAs a Data Engineer, you will have the opportunity to:\n• Create and maintain optimal data pipeline architecture.\n• Assemble large, complex data sets that meet functional / non-functional business requirements utilizing both cloud and SAP toolsets.\n• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\n• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, SAP, and Azure technologies.\n• Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization.\n• Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes.\n• Write unit/integration tests, contribute to operational and design repositories.\n• Perform data analysis required to troubleshoot data-related issues and assist in the resolution of data issues.\n• Define and catalog company data assets, data models, and pipeline jobs.\n• Design data integrations and data quality framework.\n• Work closely with all business units and engineering teams to develop strategy for long term data platform architecture.\n\nWe are seeking a candidate who meets the following qualifications:\n• Bachelor's degree in Computer Science, Computer Engineering, or a related discipline.\n• 4+ year's experience as a data engineer.\n• Python/PySpark experience.\n• Experience in engaging with stakeholders/business partners.\n• Excellent analytical and problem-solving skills.\n• Experience designing, building, and maintaining data processing systems.\n• Knowledge of data analytics, data mapping, data modeling, and data profiling tools.\n• Retail experience is an asset.\n• SAP and Azure experience are a plus.\n\n#LI-Hybrid #LI-LM1 #analyticsatsobeys\n\nWe offer teammates competitive total compensation packages that will vary by role and location. Some websites share our job opportunities and may provide salary estimates without our knowledge. These estimates are based on similar jobs and postings for general comparison, but these numbers are not provided by or monitored for accuracy by our organization. We look forward to discussing the specific compensation details relevant to this role with candidates who are selected to move forward in the recruitment process.\n\nSobeys is committed to accommodating applicants with disabilities throughout the hiring process and will work with applicants requesting accommodation at any stage of this process.\n\nWhile all responses are appreciated only those being considered for interviews will be acknowledged.\n\nWe appreciate the interest from the Staffing industry however respectfully request no calls or unsolicited resumes from Agencies.", 'Job Description\n\nPython\n\nSQL\nData modelling experience\n\nAWS\nAdditional Information\n\nCandidates only Physically in Canada having Valid Canadian Work Authorization will be considered. NO VISA SPONSORSHIP will be provided.'], ['Toronto', 'Toronto', 'Toronto', 'Oakville', 'Toronto', 'Waterloo', 'Mississauga', 'Toronto', 'Toronto', 'Toronto'], ['CA', 'CA', 'CA', 'CA', 'CA', 'CA', 'CA', 'CA', 'CA', 'CA'], ['2023-08-14', '2023-08-14', '2023-08-15', '2023-08-14', '2023-08-09', '2023-08-12', '2023-08-15', '2023-08-14', '2023-08-09', '2023-08-15'], ['Consulting', 'Education', None, None, None, 'Finance', 'Finance', None, 'Retail', None])
[2023-08-16T09:07:04.634+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=rapid_api_v1, task_id=extract_relevant_records, execution_date=20230811T000000, start_date=20230816T090704, end_date=20230816T090704
[2023-08-16T09:07:04.685+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-08-16T09:07:04.711+0000] {taskinstance.py:2582} INFO - 1 downstream tasks scheduled from follow-on schedule check
